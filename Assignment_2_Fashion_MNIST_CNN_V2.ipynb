{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment 2: Fashion MNIST CNN - V2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EtiPeretz/Fashion-MNIST-CNN/blob/main/Assignment_2_Fashion_MNIST_CNN_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYgpzGuMdx8-"
      },
      "source": [
        "**Assignment # 2, CNN over Fasion MNIST**\n",
        "\n",
        "In this assignment you are required to develop a convolutional network and train it over the Fasion MNIST data, a collection of 28X28 black and white images, classified into 10 different classes of clothing items. For more information about Fashion MNIST you may refer to: \n",
        "https://github.com/zalandoresearch/fashion-mnist "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5w6wIzcd1LG"
      },
      "source": [
        "# Loading Fashion MNIST\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transforms.ToTensor())\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transforms.ToTensor())\n",
        "\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
        "           'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xs3rDG0xBmL"
      },
      "source": [
        "# Use dataloaders for train and test (batch size is 4)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKOHObeAj99B",
        "outputId": "3acd6658-84ae-4dbd-dc34-282fa595a7ae"
      },
      "source": [
        "# The images are of 1, 28, 28 size (only one black-white channel)\n",
        "\n",
        "trainset[0][0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6jH2cjTVnVi"
      },
      "source": [
        "# **Part 1**: Implementing a CNN network for Fashion MNIST\n",
        "Here is what you need to do; you are encoureged to look at notebook \"Notebook 11 - CIFAR CNN\" when trying to complete the next steps.\n",
        "\n",
        "\n",
        "Write a network CNNFMnist, that has the following architecture:\n",
        "\n",
        "* Convolution with 10 3X3 filters\n",
        "* Relu\n",
        "* Max pool with 2X2\n",
        "* Convolution with 5 3X3 filters\n",
        "* Relu\n",
        "* Convolution with 16 3X3 filters\n",
        "* Relu\n",
        "* Max pool with 2X2\n",
        "* Liner, output size 128\n",
        "* Relu\n",
        "* Liner, output size 64\n",
        "* Relu\n",
        "* Liner, output size 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoiGc1-donFO"
      },
      "source": [
        "class CNNFMnist(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNFMnist, self).__init__()\n",
        "        # define layers\n",
        "        # define Convolution with 10 3X3 filters\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3)\n",
        "        # define Convolution with 5 3x3 filters\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=5 , kernel_size=3)\n",
        "        # define Concolution with 16 3x3 filters \n",
        "        self.conv3 = nn.Conv2d(in_channels=5, out_channels=16 , kernel_size=3)\n",
        "        # Liner, output size 128\n",
        "        self.fc1 = nn.Linear(in_features=256, out_features=128)\n",
        "        # Liner, output size 64\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
        "        # Liner, output size 10\n",
        "        self.out = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "        # define loss function - use cross entropy\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        # define the optimizer - use SGD\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum = 0.9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window and Relu\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # Relu\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # Max pooling over a (2, 2) window and Relu\n",
        "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        #print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        # Relu\n",
        "        x = F.relu(x)\n",
        "        # Relu\n",
        "        x = F.relu(self.fc2(x))\n",
        "        out = self.out(x)\n",
        "        # Modify your CNNFMnist implementation to return the output of the \n",
        "        # layer one before last after Relu in addition to the final output\n",
        "        return out, x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksWPM9kvYWmC"
      },
      "source": [
        "Write a code that trains the network with the FashionMNIST training dataset, for classification (use cross entropy and SGD).\n",
        "Run the network for at least 10 epochs, over the entire dataset. Make sure to print the loss over the train set as well as the **test set** over time (say, every 1000 batches, but it's up to you), so you will know where you are during training. \n",
        "\n",
        "Note, measuring loss of test is similar to measuring loss over the train test. However, make sure not to run the test images in back propagation. Use them only in forward and calulate the average loss over the entire test set. Since it will make the training process run slower, you should measure loss for the test set only at the end of an epoch (so overall you get 10 loss values for the test set). You are encoureged to write a different function for claculating the loss of the test set, and then call it from the training procedure.\n",
        "\n",
        "\n",
        "You should collect the loss values in an array, so you can plot then into two curves, one for train and one for test.\n",
        "\n",
        "In addition, you should measure the time it takes you to train the network completely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ7qX52gY5-a"
      },
      "source": [
        "def train(net, is_gpu, epoch):\n",
        "  # training loop\n",
        "  print('Start Training')\n",
        "  # define loss function - use cross entropy\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  running_loss = 0.0\n",
        "  average_train_loss = 0.0\n",
        "  total = len(trainloader.dataset)\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    \n",
        "    if is_gpu:\n",
        "      inputs = inputs.cuda() # -- For GPU\n",
        "      labels = labels.cuda() # -- For GPU\n",
        "\n",
        "    # zero the parameter gradient\n",
        "    net.optimizer.zero_grad()\n",
        "    \n",
        "    # forward + backward + optimize\n",
        "    _, outputs = net(inputs)\n",
        "    # use cross entropy\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    # use SGD\n",
        "    net.optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    average_train_loss += loss.item()\n",
        "    if (i+1) % 1000 == 0:\n",
        "            # Make sure to print the loss over the train set over time\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 1000))\n",
        "            running_loss = 0.0\n",
        "  # calculate the average loss entire data set\n",
        "  average_train_loss /= total\n",
        "  return average_train_loss\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa0sxqAhY8wA"
      },
      "source": [
        "Write a function that evaluates the resulted model over the entire test data of FashionMNIST. Provide a single accuracy number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzuGund9aBOp"
      },
      "source": [
        "def test(net, is_gpu):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss_of_test = 0\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # use the GPU instead of the CPU\n",
        "        if is_gpu:\n",
        "          images = images.cuda()  # -- for GPU\n",
        "          labels = labels.cuda()  # -- for GPU\n",
        "        _, outputs = net(images)\n",
        "        loss_of_test += net.criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "  # calulate the average loss over the entire test set\n",
        "  loss_of_test /= len(testloader.dataset)\n",
        "  # Make sure to print the loss over the test set over time\n",
        "  print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
        "  return loss_of_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMnb9gSGaIjP"
      },
      "source": [
        "# **Part 2**: Training with a GPU \n",
        "You are requested to change your code to use the GPU instead of the CPU.\n",
        "This can be easily done bu converting every torch.tensor to torch.cuda.tensor. \n",
        "\n",
        "Specific instructions:\n",
        "* Change the hardware equipent of your colab notebook. To do that, go to the \"Runtime\" menu, and then to \"Change runtime type\". In the dialog box, change \"Hardware accelerator\" to GPU.\n",
        "* Please follow the lines that were commented out with the comment    # -- For GPU\n",
        "* Also, remove the lines that have the comment # -- For CPU\n",
        "\n",
        "Train your network again, compare and report training times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFP4xEh3ENR2"
      },
      "source": [
        "def eval(net):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "      images, labels = data\n",
        "      if is_gpu:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs, _ = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0) \n",
        "        correct += (predicted == labels).sum().item()\n",
        "  print('Accuracy of the network on the test images of FashionMnist: %d %%' % (100 * correct / total))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFERC6VPD1L7"
      },
      "source": [
        "def part1_and_part2():\n",
        "    if is_gpu == True:\n",
        "      net = CNNFMnist().cuda()\n",
        "      type = \"GPU\"\n",
        "    else:\n",
        "      net = CNNFMnist()   \n",
        "      type = \"CPU\"\n",
        "    losses_of_train = []\n",
        "    losses_of_test = [] \n",
        "\n",
        "    # measure the time it takes you to train the network completely\n",
        "    start_time = time.time()\n",
        "    # Run the network for at least 10 epochs\n",
        "    for epoch in range(10):\n",
        "      losses_of_train.append(train(net, is_gpu, epoch))\n",
        "      losses_of_test.append(test(net, is_gpu))\n",
        "    # plot then into two curves\n",
        "    # one for train\n",
        "    plt.plot(losses_of_train, label=\"train\", color=\"blue\")\n",
        "    # one for test\n",
        "    plt.plot(losses_of_test, label=\"test\", color=\"red\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    # the end of the time it took to train the network\n",
        "    end_time = time.time()\n",
        "    total_time = end_time-start_time \n",
        "    print(\"By using %s takes %d.\" % (type, total_time))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdYn8JXyLk07"
      },
      "source": [
        "# **Part 3**: Transfer Learning\n",
        "Training data is a valuable resource, and sometimes there is not enough of it for training a neural netowrk at scale. To handle this situation, one approach is transfer learning, where we train our network on a different related task, and then switch to train it on the downstream task that we focus on. In this last part of the assignment, you are requested to pretrain your network on CIFAR-10, then train it on Fashion-MNIST, and measure its contribution to the results. To do that, please follow the steps:\n",
        "\n",
        "**Step 1**\n",
        "\n",
        "Modify your CNNFMnist implementation to return the output of the layer one before last after Relu (Linear layer of size 64, above) in addition to the final output. For example:\n",
        "\n",
        "```\n",
        "def forward(self, x):\n",
        "  ...\n",
        "  return h, out\n",
        "```\n",
        "\n",
        " and train it on the training-set part of CIFAR-10. Use batch size of 4, and train it for at least 10 epochs. Note that CIFAR-10 images are of different shapes (3X32X32), therefore a conversion into 1X28X28 is needed. To do that, when you load CIFAR-10 using a torchvision Dataset, you can use the transformer torchvision.transforms.Grayscale(num_output_channels=1) in order to convert the images to a 1X32X32 grayscale volume:\n",
        "\n",
        "```\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=torchvision.transforms.Compose([torchvision.transforms.Grayscale(num_output_channels=1),\n",
        "                                    torchvision.transforms.ToTensor()]))\n",
        "```\n",
        "Then, from each 1X32X32 image, sample 10 1X28X28 images at random positions, and use them for training (*optional* - for data augmentation, if you want, you can also generate the reflection of each of the 10 images and add them the training set).\n",
        "\n",
        "**Setp 2**\n",
        "\n",
        "Once done, write a new Module CNNFMnist2, which uses CNNFMnist as one of its sub modules, followed by some additional layers. The output of CNNFMnist that goes into the next layer, should be the output of the 64 neuron one-before-last layer, as described above. CNNFMnist2 should have the following architecture:\n",
        "\n",
        "* CNNFMnist\n",
        "* Liner, output size 32\n",
        "* Relu\n",
        "* Liner, output size 16\n",
        "* Relu\n",
        "* Liner, output size 10\n",
        "\n",
        "Make sure to allow the user to assign a pre-trained version CNNFMnist as a member of the module. For example:\n",
        "\n",
        "```\n",
        "class CNNFMnist2(nn.Module):\n",
        "    def __init__(self, trained_cnnfmnist_model):\n",
        "        super(CNNFMnist2, self).__init__()\n",
        "        self.trained_cnnfmnist_model = trained_cnnfmnist_model\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        ...\n",
        "```\n",
        "\n",
        "**Step 3**\n",
        "\n",
        "Train and eval CNNFMnist2 on Fashion-MNIST a few times:\n",
        "- Using the pre-trained version of CNNFMnist.\n",
        "- Using a fresh CNNFMnist instance (without training it).\n",
        "- (Optional) Using the pre-trained version of CNNFMnist, after freezing its weights using the .eval() function.\n",
        "\n",
        "Report on evaluation results (accuracy) for all of those cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQlnPT7gP-Wb"
      },
      "source": [
        "# Part 3, Step 2: new Module CNNFMnist2\n",
        "# which uses CNNFMnist as one of its sub modules, followed by some additional layers.\n",
        "class CNNFMnist2(nn.Module):\n",
        "\n",
        "    def __init__(self, trained_cnnfmnist_model):\n",
        "        super(CNNFMnist2, self).__init__()\n",
        "        # Make sure to allow the user to assign a pre-trained version CNNFMnist as a member of the module.\n",
        "        self.trained_cnnfmnist_model = trained_cnnfmnist_model\n",
        "        # Liner, output size 32\n",
        "        self.fc1 = nn.Linear(in_features=64, out_features=32)\n",
        "        # Liner, output size 16\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=16)\n",
        "        # Liner, output size 10\n",
        "        self.out = nn.Linear(in_features=16, out_features=10)\n",
        "\n",
        "        # define loss function - use cross entropy\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        # define the optimizer - use SGD\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, x = self.trained_cnnfmnist_model(x)\n",
        "        # Relu\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # Relu\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "        return x, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dlHejXuMo2a"
      },
      "source": [
        "def train_of_part3(model, dataloader, epoch):\n",
        "  # training loop\n",
        "  # define loss function - use cross entropy\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  running_loss = 0.0\n",
        "  average_train_loss = 0.0\n",
        "  total = len(trainloader.dataset)\n",
        "  for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        \n",
        "        if is_gpu:\n",
        "          inputs = inputs.cuda() # -- For GPU\n",
        "          labels = labels.cuda() # -- For GPU\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        model.optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs, _ = model(inputs)\n",
        "        loss = model.criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        average_train_loss += loss.item()\n",
        "        if (i+1) % 1000 == 0:    \n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 1000))\n",
        "            running_loss = 0.0\n",
        "  # calculate the average loss entire data set\n",
        "  average_train_loss /= total\n",
        "  return average_train_loss, model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzJ1eu-NvYfR"
      },
      "source": [
        "def part3():\n",
        "  # Step 1\n",
        "  trainPart3 = []\n",
        "  # Train and eval CNNFMnist2 on Fashion-MNIST a few times:\n",
        "  # train it for at least 10 epochs.\n",
        "  for i in range(10):\n",
        "    # train it on the training-set part of CIFAR-10\n",
        "    train_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.Grayscale(num_output_channels=1),transforms.CenterCrop(28),\n",
        "                                    torchvision.transforms.ToTensor()]))\n",
        "    trainPart3 = train_cifar10 + trainPart3\n",
        "  train_loader_of_CIFAR10 = torch.utils.data.DataLoader(trainPart3, batch_size=64, shuffle=True)\n",
        "  print(trainPart3[0][0].shape)\n",
        "\n",
        "  # pre-trained version of CNNFMnist\n",
        "  model_of_CNNFMnist = CNNFMnist().cuda()\n",
        "\n",
        "  # pretrain your network on CIFAR-10\n",
        "  for epoch in range(10):\n",
        "    _, pre_train = train_of_part3(model_of_CNNFMnist, train_loader_of_CIFAR10, epoch)\n",
        "\n",
        "  # Step 3: Train and eval CNNFMnist2 on Fashion-MNIST a few times:\n",
        "  # - Using the pre-trained version of CNNFMnist.\n",
        "  CNNFMnist_model2 = CNNFMnist2(pre_train).cuda()\n",
        "  for epoch in range(10):\n",
        "    _ = train(CNNFMnist_model2, is_gpu, epoch)\n",
        "\n",
        "  eval(CNNFMnist_model2)\n",
        "\n",
        "  # Using a fresh CNNFMnist instance\n",
        "  print(\"fresh CNNFMnist\")\n",
        "  cnnmnist = CNNFMnist().cuda()\n",
        "  cnnmnist2 = CNNFMnist2(cnnmnist).cuda()\n",
        "  for epoch in range(10):\n",
        "    _, net = train_of_part3(cnnmnist2, trainloader, epoch)\n",
        "\n",
        "  eval(net)\n",
        "\n",
        "  print(\"The Assignment2 is ended\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zUZRNM1-8Q9f",
        "outputId": "8ac25d09-5bd7-42d2-aec7-21f2b1228613"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  is_gpu = True\n",
        "  part1_and_part2()\n",
        "  part3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "[1,  1000] loss: 2.980\n",
            "[1,  2000] loss: 1.205\n",
            "[1,  3000] loss: 0.834\n",
            "[1,  4000] loss: 0.702\n",
            "[1,  5000] loss: 0.698\n",
            "[1,  6000] loss: 0.633\n",
            "[1,  7000] loss: 0.613\n",
            "[1,  8000] loss: 0.595\n",
            "[1,  9000] loss: 0.568\n",
            "[1, 10000] loss: 0.560\n",
            "[1, 11000] loss: 0.556\n",
            "[1, 12000] loss: 0.485\n",
            "[1, 13000] loss: 0.483\n",
            "[1, 14000] loss: 0.473\n",
            "[1, 15000] loss: 0.465\n",
            "Accuracy of the network on the test images: 82 %\n",
            "Start Training\n",
            "[2,  1000] loss: 0.454\n",
            "[2,  2000] loss: 0.449\n",
            "[2,  3000] loss: 0.436\n",
            "[2,  4000] loss: 0.435\n",
            "[2,  5000] loss: 0.455\n",
            "[2,  6000] loss: 0.452\n",
            "[2,  7000] loss: 0.423\n",
            "[2,  8000] loss: 0.418\n",
            "[2,  9000] loss: 0.414\n",
            "[2, 10000] loss: 0.407\n",
            "[2, 11000] loss: 0.429\n",
            "[2, 12000] loss: 0.396\n",
            "[2, 13000] loss: 0.399\n",
            "[2, 14000] loss: 0.388\n",
            "[2, 15000] loss: 0.386\n",
            "Accuracy of the network on the test images: 84 %\n",
            "Start Training\n",
            "[3,  1000] loss: 0.383\n",
            "[3,  2000] loss: 0.386\n",
            "[3,  3000] loss: 0.346\n",
            "[3,  4000] loss: 0.384\n",
            "[3,  5000] loss: 0.346\n",
            "[3,  6000] loss: 0.368\n",
            "[3,  7000] loss: 0.361\n",
            "[3,  8000] loss: 0.366\n",
            "[3,  9000] loss: 0.366\n",
            "[3, 10000] loss: 0.392\n",
            "[3, 11000] loss: 0.382\n",
            "[3, 12000] loss: 0.357\n",
            "[3, 13000] loss: 0.362\n",
            "[3, 14000] loss: 0.337\n",
            "[3, 15000] loss: 0.362\n",
            "Accuracy of the network on the test images: 86 %\n",
            "Start Training\n",
            "[4,  1000] loss: 0.345\n",
            "[4,  2000] loss: 0.334\n",
            "[4,  3000] loss: 0.357\n",
            "[4,  4000] loss: 0.327\n",
            "[4,  5000] loss: 0.345\n",
            "[4,  6000] loss: 0.306\n",
            "[4,  7000] loss: 0.358\n",
            "[4,  8000] loss: 0.359\n",
            "[4,  9000] loss: 0.347\n",
            "[4, 10000] loss: 0.332\n",
            "[4, 11000] loss: 0.318\n",
            "[4, 12000] loss: 0.341\n",
            "[4, 13000] loss: 0.327\n",
            "[4, 14000] loss: 0.334\n",
            "[4, 15000] loss: 0.328\n",
            "Accuracy of the network on the test images: 87 %\n",
            "Start Training\n",
            "[5,  1000] loss: 0.326\n",
            "[5,  2000] loss: 0.317\n",
            "[5,  3000] loss: 0.329\n",
            "[5,  4000] loss: 0.309\n",
            "[5,  5000] loss: 0.346\n",
            "[5,  6000] loss: 0.306\n",
            "[5,  7000] loss: 0.316\n",
            "[5,  8000] loss: 0.309\n",
            "[5,  9000] loss: 0.304\n",
            "[5, 10000] loss: 0.300\n",
            "[5, 11000] loss: 0.336\n",
            "[5, 12000] loss: 0.326\n",
            "[5, 13000] loss: 0.297\n",
            "[5, 14000] loss: 0.297\n",
            "[5, 15000] loss: 0.313\n",
            "Accuracy of the network on the test images: 85 %\n",
            "Start Training\n",
            "[6,  1000] loss: 0.308\n",
            "[6,  2000] loss: 0.290\n",
            "[6,  3000] loss: 0.293\n",
            "[6,  4000] loss: 0.288\n",
            "[6,  5000] loss: 0.290\n",
            "[6,  6000] loss: 0.306\n",
            "[6,  7000] loss: 0.282\n",
            "[6,  8000] loss: 0.296\n",
            "[6,  9000] loss: 0.328\n",
            "[6, 10000] loss: 0.300\n",
            "[6, 11000] loss: 0.302\n",
            "[6, 12000] loss: 0.280\n",
            "[6, 13000] loss: 0.296\n",
            "[6, 14000] loss: 0.306\n",
            "[6, 15000] loss: 0.318\n",
            "Accuracy of the network on the test images: 88 %\n",
            "Start Training\n",
            "[7,  1000] loss: 0.269\n",
            "[7,  2000] loss: 0.300\n",
            "[7,  3000] loss: 0.285\n",
            "[7,  4000] loss: 0.253\n",
            "[7,  5000] loss: 0.278\n",
            "[7,  6000] loss: 0.306\n",
            "[7,  7000] loss: 0.291\n",
            "[7,  8000] loss: 0.301\n",
            "[7,  9000] loss: 0.298\n",
            "[7, 10000] loss: 0.295\n",
            "[7, 11000] loss: 0.279\n",
            "[7, 12000] loss: 0.292\n",
            "[7, 13000] loss: 0.286\n",
            "[7, 14000] loss: 0.303\n",
            "[7, 15000] loss: 0.285\n",
            "Accuracy of the network on the test images: 88 %\n",
            "Start Training\n",
            "[8,  1000] loss: 0.270\n",
            "[8,  2000] loss: 0.278\n",
            "[8,  3000] loss: 0.283\n",
            "[8,  4000] loss: 0.280\n",
            "[8,  5000] loss: 0.263\n",
            "[8,  6000] loss: 0.264\n",
            "[8,  7000] loss: 0.279\n",
            "[8,  8000] loss: 0.280\n",
            "[8,  9000] loss: 0.292\n",
            "[8, 10000] loss: 0.267\n",
            "[8, 11000] loss: 0.268\n",
            "[8, 12000] loss: 0.287\n",
            "[8, 13000] loss: 0.294\n",
            "[8, 14000] loss: 0.271\n",
            "[8, 15000] loss: 0.277\n",
            "Accuracy of the network on the test images: 88 %\n",
            "Start Training\n",
            "[9,  1000] loss: 0.263\n",
            "[9,  2000] loss: 0.262\n",
            "[9,  3000] loss: 0.256\n",
            "[9,  4000] loss: 0.266\n",
            "[9,  5000] loss: 0.267\n",
            "[9,  6000] loss: 0.280\n",
            "[9,  7000] loss: 0.247\n",
            "[9,  8000] loss: 0.281\n",
            "[9,  9000] loss: 0.259\n",
            "[9, 10000] loss: 0.267\n",
            "[9, 11000] loss: 0.287\n",
            "[9, 12000] loss: 0.269\n",
            "[9, 13000] loss: 0.277\n",
            "[9, 14000] loss: 0.255\n",
            "[9, 15000] loss: 0.284\n",
            "Accuracy of the network on the test images: 88 %\n",
            "Start Training\n",
            "[10,  1000] loss: 0.245\n",
            "[10,  2000] loss: 0.248\n",
            "[10,  3000] loss: 0.250\n",
            "[10,  4000] loss: 0.264\n",
            "[10,  5000] loss: 0.261\n",
            "[10,  6000] loss: 0.269\n",
            "[10,  7000] loss: 0.261\n",
            "[10,  8000] loss: 0.262\n",
            "[10,  9000] loss: 0.250\n",
            "[10, 10000] loss: 0.239\n",
            "[10, 11000] loss: 0.247\n",
            "[10, 12000] loss: 0.289\n",
            "[10, 13000] loss: 0.252\n",
            "[10, 14000] loss: 0.280\n",
            "[10, 15000] loss: 0.274\n",
            "Accuracy of the network on the test images: 88 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "By using CPU takes 423.\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-865b99c32163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mis_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpart1_and_part2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpart3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-d300601332dc>\u001b[0m in \u001b[0;36mpart3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# pretrain your network on CIFAR-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_of_part3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_of_CNNFMnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_of_CIFAR10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# Step 3: Train and eval CNNFMnist2 on Fashion-MNIST a few times:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-9671641d8c0c>\u001b[0m in \u001b[0;36mtrain_of_part3\u001b[0;34m(model, dataloader, epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-7965ba57d2a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Max pooling over a (2, 2) window and Relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm5DnMate6s0"
      },
      "source": [
        "# Submission instructions\n",
        "\n",
        "You should submit a pdf file with the following items:\n",
        "\n",
        "First Name Last Name, ID\n",
        "\n",
        "First Name2 Last Name2, ID\n",
        "\n",
        "Link to Colab\n",
        "\n",
        "\n",
        "\n",
        "CPU Experiment:\n",
        "*   Plot of loss curves (train in blue, test in red)\n",
        "*   Training time\n",
        "\n",
        "GPU Experiment:\n",
        "*   Plot of loss curves (train in blue, test in red)\n",
        "*   Training time\n",
        "\n",
        "Transfer Learning Experiment:\n",
        "* Accuracy results on test set for the 2-3 implemeted settings (see above)\n",
        "\n",
        "\n",
        "\n",
        "Good luck!"
      ]
    }
  ]
}